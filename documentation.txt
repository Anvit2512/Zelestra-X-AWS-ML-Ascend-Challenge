Solar Panel Efficiency Prediction Approach
Team name-Visionaries
Members- Aryan Saxena, Anvit Kumar



1. Project overview & modelling approach
To achieve the highest possible accuracy and robustness, a two level stacked ensemble model was developed. This approach outperforms any single model by intelligently combining the predictions of several diverse base models. The core philosophy is that by combining different models, the strengths of one can compensate for the weaknesses of another, leading to a more accurate, resistant to overfitting and generalizable final prediction.
The architecture is as follows:
	A. Level 0 (Base Models): A committee of four different, powerful machine learning models are trained on the full set of engineered features. Each model learns to 	predict the target (efficiency) from a different "perspective."
	B. Level 1 (Meta-Model): A simple, regularized linear model is trained on the predictions generated by the Level 0 models. Its sole job is to learn the optimal 	weights to combine the outputs from the "committee of experts" into a single, final prediction.



2. Feature engineering details

2.1. Data cleaning and imputation
a) Action: All original numeric columns were processed to handle non numeric entries (e.g, the string 'error'). These entries were converted to NaN (Not a Number).
b) Imputation Strategy: All resulting NaN values were then filled with the median of their respective column.
c) Purpose: The median is used instead of the mean as it is more robust to extreme outliers, providing a more stable and representative value for missing data. This ensures the dataset is clean and complete before training.

2.2. Physics based and interaction features
a) power: Calculated as voltage * current
b) temp_diff: Calculated as module_temperature - temperature. This feature captures the thermal gradient, which can significantly impact electronic efficiency.

2.3. Group based statistical features
a) Action: For each categorical column (string_id, error_code, installation_type), we calculated the mean, standard deviation, and max of the most important numeric features (irradiance, temperature, power, temp_diff).
b) Purpose: This provides crucial context. For any given data point, the model now knows not just its own values, but also the typical operating conditions and volatility of the group it belongs to. For example, it knows the average power generated by all panels in a specific string_id.

2.4. Deviation from group mean features
a) Action: For each key numeric feature, a new feature was created representing its difference from the group level mean (e.g., irradiance - mean_irradiance_for_this_string_id).
b) Purpose: This is a powerful form of normalization that tells the model how unusual a specific reading is compared to its group's average. An abnormally high or low reading is often a strong predictor of a change in efficiency.

2.5. Polynomial interaction features
a) Action: Using sklearn.preprocessing.PolynomialFeatures, second degree interaction terms were created for the most critical variables (irradiance, temperature, power, temp_diff). This creates features like irradiance * temperature.
b) Purpose: While tree based models can learn interactions implicitly, explicitly providing these terms helps them model complex, non linear physical relationships more easily and efficiently.



3. Model architecture in detail

3.1. Level 0: Base models
A diverse set of four models was chosen to ensure a wide range of algorithmic approaches. All models are trained using a 10 fold cross validation strategy to generate robust out of fold (OOF) predictions for the next level.
a) LightGBM:
	i. Fast, distributed gradient boosting framework
	ii. Key parameters: num_leaves=32, learning_rate=0.01, early stoppin
b) CatBoost:
	i. Native handling of categorical features
	ii. Key parameters: depth=8, l2_leaf_reg=3, built-in categorical support
c) XGBoost:
	i. Optimized gradient boosting
	ii. Key parameters: max_depth=6, tree_method='hist'
d) ExtraTrees:
	i. Extremely Randomized Trees
	ii. Provides diversity to the ensemble
	iii. Key parameters: max_depth=15, min_samples_leaf=5

3.2. Level 1: Ridge Regression
a) Simple linear combination of base model predictions
b) Prevents overfitting through L2 regularization
c) Automatically determines optimal weights for each base model



4. Post processing
a) Prediction clipping: The final predictions generated by the Ridge meta-model are clipped to fall within the minimum and maximum efficiency values observed in the original training data. 
b) This is a crucial safety step to ensure all predictions are physically plausible and within a realistic range.



5. Tools and libraries used
a) Core libraries:
	i. Pandas: For data manipulation, reading CSV files, and DataFrame management.
	ii. NumPy: For numerical operations, especially on arrays for model predictions.
b) Machine learning & modeling:
	i. Scikit learn: For core utilities including:
		- KFold for cross-validation.
		- Ridge and ExtraTreesRegressor models.
		- LabelEncoder and PolynomialFeatures for preprocessing.
		- mean_squared_error for evaluation.
	ii. LightGBM: For the LGBMRegressor model.
	iii. XGBoost: For the XGBRegressor model.
	iv. CatBoost: For the CatBoostRegressor model.
c) Utilities:
	i. warnings: To suppress unnecessary warnings during execution.
	ii. gc (garbage collector): To manually free up memory during the intensive model training loops.



6. Source files
a) Main script: solar_efficiency_prediction.py
	i. Contains the complete implementation
	ii. Structured as a sequential pipeline with clear sections
b) Configuration:
	i. All parameters centralized in the CFG class
	ii. Includes model hyperparameters and global settings
c) Data files:
	i. train.csv: Training data with features and target
	ii. test.csv: Test data for final predictions



7. Performance metrics
a) Out of Fold RMSE: 0.102489
b) Expected competition score: 89.751126
c) Final submission score: 89.91144



8. Key insights
a) Feature importance:
 	i. Deviation features proved most valuable
	ii. Group statistics provided critical context
b) Model contributions:
	i. LightGBM and CatBoost carried most weight (58% and 44% respectively)
	ii. XGBoost and ExtraTrees provided minor adjustments
c) Stability factors:
	i. Early stopping prevented overfitting
	ii. K-Fold validation ensured reliable estimates
	iii. Clipping maintained physical plausibility

