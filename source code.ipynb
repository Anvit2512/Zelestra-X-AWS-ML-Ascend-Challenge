{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdafc74-31e0-42cf-bdf1-ec921f1020d8",
   "metadata": {},
   "source": [
    "down one is main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c467a53-cde9-4e83-884a-9056d12166f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Installing required libraries...\n",
      "\n",
      "Step 1: Importing libraries and setting up configuration...\n",
      "\n",
      "Step 2: Loading data...\n",
      "Data loaded successfully!\n",
      "\n",
      "Step 4: Preparing data for different models...\n",
      "Feature shapes: X_lgb_train=(20000, 16), X_cat_train=(20000, 16), X_xgb_train=(20000, 16)\n",
      "\n",
      "Data type check for CatBoost training data:\n",
      "float64    12\n",
      "object      3\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Step 5: Training Level 0 Base Models...\n",
      "\n",
      "--- Training LightGBM ---\n",
      "\n",
      "--- Training CatBoost ---\n",
      "\n",
      "--- Training XGBoost ---\n",
      "\n",
      "--- Training ExtraTrees ---\n",
      "\n",
      "Step 6: Evaluating Base Models and Training Ridge Meta-Model...\n",
      "LGB OOF RMSE: 0.103585\n",
      "CAT OOF RMSE: 0.102761\n",
      "XGB OOF RMSE: 0.104053\n",
      "ET OOF RMSE: 0.108968\n",
      "\n",
      "Final Stacked OOF RMSE: 0.102489\n",
      "Final Expected Competition Score: 89.751126\n",
      "Meta-Model (Ridge) Coefficients: LGB=0.5794, CAT=0.4444, XGB=0.0013, ET=-0.0071\n",
      "\n",
      "Step 7: Creating final submission file with clipping...\n",
      "\n",
      "Robust ensemble submission file 'submission_robust_ensemble.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# Step 0: Install Required Libraries\n",
    "# ==============================================================================\n",
    "print(\"Step 0: Installing required libraries...\")\n",
    "!pip install category_encoders -q\n",
    "!pip install catboost -q\n",
    "# Pinning to a known, stable version to ensure API consistency\n",
    "!pip install xgboost==1.7.6 -q\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 1: Library Imports and Configuration\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 1: Importing libraries and setting up configuration...\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration Block ---\n",
    "class CFG:\n",
    "    N_SPLITS = 10\n",
    "    RANDOM_STATE = 42\n",
    "    TARGET_COL = 'efficiency'\n",
    "    ID_COL = 'id'\n",
    "    \n",
    "    CAT_COLS = ['string_id', 'error_code', 'installation_type']\n",
    "    # Define only the *original* numeric columns here for cleaning\n",
    "    ORIGINAL_NUM_COLS = ['irradiance', 'temperature', 'voltage', 'current', 'humidity', \n",
    "                         'wind_speed', 'pressure', 'soiling_ratio', 'module_temperature']\n",
    "\n",
    "    LGBM_PARAMS = {\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 10000, \n",
    "        'learning_rate': 0.01, 'num_leaves': 32, 'max_depth': 7, 'seed': RANDOM_STATE, \n",
    "        'n_jobs': -1, 'verbose': -1, 'colsample_bytree': 0.7, 'subsample': 0.7, \n",
    "        'reg_alpha': 0.1, 'reg_lambda': 0.1\n",
    "    }\n",
    "    CAT_PARAMS = {\n",
    "        'iterations': 10000, 'learning_rate': 0.02, 'depth': 8, 'loss_function': 'RMSE', \n",
    "        'eval_metric': 'RMSE', 'random_seed': RANDOM_STATE, 'verbose': 0, \n",
    "        'early_stopping_rounds': 200, 'l2_leaf_reg': 3\n",
    "    }\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'reg:squarederror', 'eval_metric': 'rmse', 'n_estimators': 10000,\n",
    "        'learning_rate': 0.01, 'max_depth': 6, 'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7, 'random_state': RANDOM_STATE, 'n_jobs': -1,\n",
    "        'tree_method': 'hist', 'enable_categorical': True\n",
    "    }\n",
    "    ET_PARAMS = {\n",
    "        'n_estimators': 500, 'max_depth': 15, 'min_samples_leaf': 5,\n",
    "        'random_state': RANDOM_STATE, 'n_jobs': -1, 'max_features': 0.8\n",
    "    }\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 2: Load Data\n",
    "# ==============================================================================\n",
    "try:\n",
    "    print(\"\\nStep 2: Loading data...\")\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "    test_df = pd.read_csv('test.csv')\n",
    "    print(\"Data loaded successfully!\")\n",
    "    n_train = len(train_df)\n",
    "    y = train_df[CFG.TARGET_COL].copy()\n",
    "    test_ids = test_df[CFG.ID_COL].copy()\n",
    "    # Combine feature dataframes, keeping ID for now\n",
    "    full_df = pd.concat([train_df.drop(CFG.TARGET_COL, axis=1), test_df], axis=0).reset_index(drop=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"FATAL ERROR: Files not found. Please restart the runtime and re-upload your CSV files.\")\n",
    "    exit()\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 3: Feature Engineering\n",
    "# ==============================================================================\n",
    "def create_smarter_features(df):\n",
    "    print(\"\\nStep 3: Creating smarter, more robust features...\")\n",
    "    data = df.drop(CFG.ID_COL, axis=1).copy()\n",
    "    \n",
    "    # --- Robust Imputation on original numeric columns ---\n",
    "    for col in CFG.ORIGINAL_NUM_COLS:\n",
    "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "        median_val = data[col].median()\n",
    "        data[col] = data[col].fillna(median_val)\n",
    "\n",
    "    # --- Basic Interaction & Physics Features ---\n",
    "    data['power'] = data['voltage'] * data['current']\n",
    "    data['temp_diff'] = data['module_temperature'] - data['temperature']\n",
    "    \n",
    "    # --- Group-By Aggregate Features (CORRECTED FOR NEW PANDAS) ---\n",
    "    AGG_COLS = ['irradiance', 'temperature', 'power', 'temp_diff']\n",
    "    for cat_col in CFG.CAT_COLS:\n",
    "        for agg_col in AGG_COLS:\n",
    "            # Step 1: Aggregate using a list of functions\n",
    "            agg_funcs = ['mean', 'std', 'max']\n",
    "            agg_stats = data.groupby(cat_col)[agg_col].agg(agg_funcs)\n",
    "            \n",
    "            # Step 2: Create new column names and assign them\n",
    "            agg_stats.columns = [f'{cat_col}_{agg_col}_{func}' for func in agg_funcs]\n",
    "            \n",
    "            data = data.merge(agg_stats, on=cat_col, how='left')\n",
    "\n",
    "    # --- Deviation from Group Mean Features ---\n",
    "    for cat_col in CFG.CAT_COLS:\n",
    "        for agg_col in AGG_COLS:\n",
    "            # Make sure the mean column exists before trying to use it\n",
    "            mean_col_name = f'{cat_col}_{agg_col}_mean'\n",
    "            if mean_col_name in data.columns:\n",
    "                data[f'{agg_col}_vs_{cat_col}_mean'] = data[agg_col] - data[mean_col_name]\n",
    "\n",
    "    # --- Polynomial Features ---\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "    poly_features = ['irradiance', 'temperature', 'power', 'temp_diff']\n",
    "    poly_df = pd.DataFrame(poly.fit_transform(data[poly_features]), columns=poly.get_feature_names_out(poly_features))\n",
    "    # Sanitize column names from polynomial features\n",
    "    poly_df.columns = [f'poly_{c.replace(\" \", \"_\").replace(\"*\", \"_x_\")}' for c in poly_df.columns]\n",
    "    \n",
    "    data = pd.concat([data.reset_index(drop=True), poly_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    data.fillna(-999, inplace=True)\n",
    "    return data\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 4: Final Data Preparation for Models\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 4: Preparing data for different models...\")\n",
    "\n",
    "# --- Identify ALL columns that are not explicitly categorical ---\n",
    "all_cols = full_df.columns.tolist()\n",
    "final_numeric_cols = [col for col in all_cols if col not in CFG.CAT_COLS]\n",
    "\n",
    "# --- Create data for CatBoost/XGBoost ---\n",
    "X_cat = full_df.copy()\n",
    "\n",
    "# 1. Explicitly convert categorical columns to string type for CatBoost's processing\n",
    "for col in CFG.CAT_COLS:\n",
    "    X_cat[col] = X_cat[col].astype(str)\n",
    "\n",
    "# 2. Forcibly convert all other columns to numeric, coercing errors.\n",
    "for col in final_numeric_cols:\n",
    "    X_cat[col] = pd.to_numeric(X_cat[col], errors='coerce').fillna(-999)\n",
    "\n",
    "# 3. For CatBoost, it's safest to keep the categorical columns as strings/objects.\n",
    "#    For XGBoost with enable_categorical=True, it prefers the 'category' dtype. Let's make a small tweak.\n",
    "X_xgb = X_cat.copy()\n",
    "for col in CFG.CAT_COLS:\n",
    "    X_xgb[col] = X_xgb[col].astype('category')\n",
    "\n",
    "# --- Create data for LightGBM/ExtraTrees (requires integer encoding) ---\n",
    "X_lgb_et = full_df.copy()\n",
    "\n",
    "# 1. Label encode the known categorical columns\n",
    "for col in CFG.CAT_COLS:\n",
    "    le = LabelEncoder()\n",
    "    X_lgb_et[col] = le.fit_transform(X_lgb_et[col].astype(str))\n",
    "\n",
    "# 2. Forcibly convert all other columns to numeric\n",
    "for col in final_numeric_cols:\n",
    "     X_lgb_et[col] = pd.to_numeric(X_lgb_et[col], errors='coerce').fillna(-999)\n",
    "\n",
    "# Separate into Training and Test Sets\n",
    "X_lgb_train, X_lgb_test = X_lgb_et.iloc[:n_train], X_lgb_et.iloc[n_train:]\n",
    "X_cat_train, X_cat_test = X_cat.iloc[:n_train], X_cat.iloc[n_train:]\n",
    "X_xgb_train, X_xgb_test = X_xgb.iloc[:n_train], X_xgb.iloc[n_train:]\n",
    "\n",
    "print(f\"Feature shapes: X_lgb_train={X_lgb_train.shape}, X_cat_train={X_cat_train.shape}, X_xgb_train={X_xgb_train.shape}\")\n",
    "print(\"\\nData type check for CatBoost training data:\")\n",
    "print(X_cat_train.dtypes.value_counts())\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 5: Level 0 Model Training (LGBM, CatBoost, XGBoost, ExtraTrees)\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 5: Training Level 0 Base Models...\")\n",
    "kf = KFold(n_splits=CFG.N_SPLITS, shuffle=True, random_state=CFG.RANDOM_STATE)\n",
    "\n",
    "oof = np.zeros((n_train, 4))\n",
    "test_preds = np.zeros((len(test_df), 4))\n",
    "\n",
    "# Model 1: LightGBM\n",
    "print(\"\\n--- Training LightGBM ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_lgb_train, y)):\n",
    "    model = lgb.LGBMRegressor(**CFG.LGBM_PARAMS)\n",
    "    model.fit(X_lgb_train.iloc[train_idx], y.iloc[train_idx], \n",
    "              eval_set=[(X_lgb_train.iloc[val_idx], y.iloc[val_idx])], \n",
    "              callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "    oof[val_idx, 0] = model.predict(X_lgb_train.iloc[val_idx])\n",
    "    test_preds[:, 0] += model.predict(X_lgb_test) / CFG.N_SPLITS\n",
    "gc.collect()\n",
    "\n",
    "# Model 2: CatBoost\n",
    "print(\"\\n--- Training CatBoost ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_cat_train, y)):\n",
    "    model = CatBoostRegressor(**CFG.CAT_PARAMS)\n",
    "    model.fit(X_cat_train.iloc[train_idx], y.iloc[train_idx], \n",
    "              eval_set=(X_cat_train.iloc[val_idx], y.iloc[val_idx]), \n",
    "              cat_features=CFG.CAT_COLS, use_best_model=True)\n",
    "    oof[val_idx, 1] = model.predict(X_cat_train.iloc[val_idx])\n",
    "    test_preds[:, 1] += model.predict(X_cat_test) / CFG.N_SPLITS\n",
    "gc.collect()\n",
    "\n",
    "# Model 3: XGBoost\n",
    "print(\"\\n--- Training XGBoost ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_xgb_train, y)):\n",
    "    model = xgb.XGBRegressor(**CFG.XGB_PARAMS)\n",
    "    model.fit(X_xgb_train.iloc[train_idx], y.iloc[train_idx], \n",
    "              eval_set=[(X_xgb_train.iloc[val_idx], y.iloc[val_idx])], \n",
    "              early_stopping_rounds=200, verbose=False)\n",
    "    oof[val_idx, 2] = model.predict(X_xgb_train.iloc[val_idx])\n",
    "    test_preds[:, 2] += model.predict(X_xgb_test) / CFG.N_SPLITS\n",
    "gc.collect()\n",
    "\n",
    "# Model 4: ExtraTrees\n",
    "print(\"\\n--- Training ExtraTrees ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_lgb_train, y)):\n",
    "    model = ExtraTreesRegressor(**CFG.ET_PARAMS)\n",
    "    model.fit(X_lgb_train.iloc[train_idx], y.iloc[train_idx])\n",
    "    oof[val_idx, 3] = model.predict(X_lgb_train.iloc[val_idx])\n",
    "    test_preds[:, 3] += model.predict(X_lgb_test) / CFG.N_SPLITS\n",
    "gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 6: Train Level 1 Meta-Model (Ridge) and Evaluate\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 6: Evaluating Base Models and Training Ridge Meta-Model...\")\n",
    "model_names = ['lgb', 'cat', 'xgb', 'et']\n",
    "for i, name in enumerate(model_names):\n",
    "    rmse = np.sqrt(mean_squared_error(y, oof[:, i]))\n",
    "    print(f\"{name.upper()} OOF RMSE: {rmse:.6f}\")\n",
    "\n",
    "stack_X_train = pd.DataFrame(oof, columns=model_names)\n",
    "stack_X_test = pd.DataFrame(test_preds, columns=model_names)\n",
    "\n",
    "meta_model = Ridge(alpha=1.0, random_state=CFG.RANDOM_STATE)\n",
    "meta_model.fit(stack_X_train, y)\n",
    "\n",
    "final_predictions = meta_model.predict(stack_X_test)\n",
    "meta_oof_preds = meta_model.predict(stack_X_train)\n",
    "stack_oof_rmse = np.sqrt(mean_squared_error(y, meta_oof_preds))\n",
    "stack_score = 100 * (1 - stack_oof_rmse)\n",
    "\n",
    "print(f\"\\nFinal Stacked OOF RMSE: {stack_oof_rmse:.6f}\")\n",
    "print(f\"Final Expected Competition Score: {stack_score:.6f}\")\n",
    "coeffs_str = \", \".join([f\"{name.upper()}={coef:.4f}\" for name, coef in zip(model_names, meta_model.coef_)])\n",
    "print(f\"Meta-Model (Ridge) Coefficients: {coeffs_str}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# Step 7: Create Final Submission with Clipping\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 7: Creating final submission file with clipping...\")\n",
    "submission_df = pd.DataFrame({CFG.ID_COL: test_ids, CFG.TARGET_COL: final_predictions})\n",
    "min_efficiency = y.min()\n",
    "max_efficiency = y.max()\n",
    "submission_df[CFG.TARGET_COL] = submission_df[CFG.TARGET_COL].clip(min_efficiency, max_efficiency)\n",
    "\n",
    "submission_df.to_csv('submission_robust_ensemble.csv', index=False)\n",
    "print(\"\\nRobust ensemble submission file 'submission_robust_ensemble.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b267f-1096-4394-abcf-6735dbdcf22c",
   "metadata": {},
   "source": [
    "this is the best of mine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e811f7-cbdb-4cbb-b095-630491c8e2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
